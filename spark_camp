Examining The SparkContext
In this exercise you'll get familiar with the SparkContext.

You'll probably notice that code takes longer to run than you might expect. This is because Spark is some serious software. 
It takes more time to start up than you might be used to. You may also find that running simpler computations might take longer than 
expected. That's because all the optimizations that Spark has under its hood are designed for complicated operations with big data sets. 
That means that for simple or small problems Spark may actually perform worse than some other solutions!

# Verify SparkContext
print(sc)

# Print Spark version
print(sc.version)

Examining The SparkContext
In this exercise you'll get familiar with the SparkContext.

You'll probably notice that code takes longer to run than you might expect. This is because Spark is some serious software. It takes more time to start up than you might be used to. 
You may also find that running simpler computations might take longer than expected. That's because all the optimizations that 
Spark has under its hood are designed for complicated operations with big data sets. That means that for simple or small problems Spark may actually perform worse than some other solutions!
# Import SparkSession from pyspark.sql

from pyspark.sql import SparkSession
# Create my_spark
my_spark = SparkSession.builder.getOrCreate()

# Print my_spark
print(my_spark)
# Print the tables in the catalog to look at the tables
print(spark.catalog.listTables())
# Don't change this query
query = "FROM flights SELECT * LIMIT 10"

# Get the first 10 rows of flights
flights10 = spark.sql(query)

# Show the results
flights10.show()
# Don't change this query
query = "SELECT origin, dest, COUNT(*) as N FROM flights GROUP BY origin, dest"
# Run the query
flight_counts = spark.sql(query)

# Convert the results to a pandas DataFrame
pd_counts = flight_counts.toPandas()

# Print the head of pd_counts
print(pd_counts.head())
---
# Create pd_temp
pd_temp = pd.DataFrame(np.random.random(10))

# Create spark_temp from pd_temp
spark_temp = spark.createDataFrame(pd_temp)

# Examine the tables in the catalog
print(spark.catalog.listTables())

# Add spark_temp to the catalog
spark_temp.createOrReplaceTempView("temp")

# Examine the tables in the catalog again
print(spark.catalog.listTables())
---
## Don't change this file path
file_path = "/usr/local/share/datasets/airports.csv"

# Read in the airports data
airports = spark.read.csv(file_path, header = True)

# Show the data
airports.show()
# Don't change this file path
file_path = "/usr/local/share/datasets/airports.csv"

# Read in the airports data
airports = spark.read.csv(file_path, header = True)

# Show the data
airports.show()

#Which of the following queries returns a table of tail numbers and destinations for flights that lasted more than 10 hours?
SELECT dest, tail_num FROM flights WHERE air_time > 600;
# SELECT dest, tail_num FROM flights WHERE air_time > 600;
SELECT AVG(air_time) / 60 FROM flights
GROUP BY origin, carrier;


